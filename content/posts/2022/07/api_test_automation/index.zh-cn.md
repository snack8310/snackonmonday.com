---
title: "自动化测试-同样的招式不要使用第二次"
date: 2022-07-20T17:25:51+08:00
draft: true
---

## 存在问题
1. 写了很多人都知道的废话。这些废话不要，要删掉。例如自动化测试价值。大家都想的出来，不需要写。
2. 可以提一下UI和接口的区别，主要是成本和收益，有部分人没有思路。
3. 缺少实现细节。具体的实现，遇到的问题。这部分别人不了解。
4. 缺少结果的评价。这部分别人不了解。
5. 视角有点低，是一个测试开发视角么？架构师视角还有什么，广度有哪些，深度有哪些。

## 想表达的内容
1. 接口自动化不难，很容易搭建
2. 接口自动化使用有点难，数据准备，过去是缺失的，系统可能也是缺失的，需要改变
3. 使用后，是会对项目流程有改变的。编译后
4. 对人员有要求，这个期间过渡成本很高
5. 使用接口测试后，短期内并不会降低测试时间，会增加，在什么部分
6. 什么场合不适用。


------------以下删除--------------


### 测试价值
略写

### 常见的测试问题
略写

### 自动化测试意义
接口
UI
差异，成本

### 自动化测试的三个层级
- UI
- 接口
- 单元测试

这三个层级，面向不同的对象，
- UI测试，面向用户。
- 接口测试，面向业务。
- 单元测试，面向研发。

越往上，测试难度越大，测试构建成本越大

是不是越往下越好。不是。
单元测试也有问题，
1. 开发量的直线上升，没有有效的目标，覆盖率？
2. 开发变化的成本高，

### 怎么做接口自动化测试
UI的是什么
接口的是什么
单元测试是什么

#### 系统构建
自动化测试框架有很多种，例如Selenium， Allure，很多大厂也有自研的。
这类框架基本特点是，脚本化语言+数据统计+报告工具。常见的Python

我们选择Jenkins+Allure+Python为例：

实践的细节。
时间的花费。
系统的改造。

```
@allure.feature('查询数据源数据')
class TestDatasource:

    # 前置条件
    @pytest.fixture(scope='class', autouse=True)
    def setup_class(cls, initialize):
        global host
        host = initialize.get('avengers-datasource')

    data_param,data_ids=read_yaml(__file__,'queryDatasource.yml')
    @allure.story('查询数据源数据')
    @pytest.mark.parametrize("args",data_param,ids=data_ids)
    def test_datasource(self,args):
        
        with allure.step('调用datasource-query接口'):
            response = DatasourceController(
                host).query_datasourceReport('6', (args['body']))
            LogHandler.info(response)
            assert response['code'] == 0,'查询数据失败'
            assert args['expected']==response['data'],'数据比对不上'
            
```

### 与传统测试的区别
没有冒烟测试
测试和开发并行
测试时间拉长-意味着探索类的项目，快速回馈的项目，并不适用。
整个项目流程的微妙区别-测试入场时间前置，测试覆盖周期拉长。
测试投入的时间大头-准备数据，准备验收结果。

### 存在的问题
测试脚本需要根据接口业务逻辑调整，成本很高
测试目录分类造成的重复开发和维护
测试覆盖率不足

### 测试驱动的区别
本质区别不大
但是避免陷入单元测试的高成本，低收益。

### 测试还是开发
经验摸索。
测试视角还是重要的。是考核视角，非答题视角。

### 接口自动化的改善关注指标
- 优化类项目的测试成本。
- 系统故障和接口测试数量。

### 如何评估收益
测试自动化是个随着功能覆盖度上升，效能逐渐提升的。
项目初期会面临时间成本变长的问题，比单纯的测试，大量的数据准备的代码落地要求，要远超过单纯的功能测试人工操作时间。
1. 测试开发比。
2. 测试事故率。


------------以下删除--------------


### 测试价值
测试很难做，因为测试的行为是与功能产出相反，并不具备做出一个事情的驱动力。
测试也很难衡量价值，因为测试自身不直接产生价值。依赖系统发现的bug数量，也许可以评价系统质量，但无法很好的评价测试价值。

善战者无赫赫之功。
非要找到一个测试价值的切入点，比较合适的方式可能是，
- 在某一个质量标准下的产品迭代速度。

能不能“保障”越来越快的迭代下不出问题，这是测试视角的价值。

如何“保障”？
是人就会出错，系统也会出错，越复杂的系统越容易出错。
想给人承诺，则需要底线的保障。
就像法律是道德的底线一样，想要维护治安，需要有一道不可逾越的法律保障。
测试就像警察一样，是维护系统质量的底线。

### 测试常见的问题
- 功能的改造，即使只增加了日志监控，也需要比较全面回归测试。开发与测试时间比不合理。
- 某一个功能的上线，莫名引起了其他功能的错误。
- 业务上线周期长。测试环境验证的内容，也许要在回归环境，预发环境再次执行。
- 一些批量的功能，类似系统对账，存在大量并发，或者时间长的情况下会出现的问题。
  - 性能测试不足，线上产生时效性的投诉。
  - 并发环境下，出现的数据不一致问题，验证不到。

这些常见的问题，要怎么解决。如何让系统警察更有效率的保障？
其中之一的手段是自动化测试。
行业里更细分的话，一般有两种：
1. 接口自动化
2. UI自动化
上述大部分内容，属于接口自动化能解决的问题。

### 自动化测试价值

自动化测试能做什么？

1. 降低重复测试成本。
套用圣斗士星矢里的话：
同样的招式，不要对一个女神的圣斗士使用两次。
自动化测试可以帮我们保障测试过的内容，不会再出错。
测试在系统升级，优化迭代的场合，有显著的效果。
2. 人力覆盖不到的点。
高并发的压力测试。
大数据规模的性能测试。
3. 人会疏忽犯错，机器不会。

### 缺点
1. 对人员要求高。
2. 测试要件准备困难。
3. 依赖业务接口的抽象能力，决定测试接口的重构频率。

### 自动化测试的粒度-需要单元测试么

- 单元测试，是开发视角，是开发对质量负责的手段。
- 接口测试，是测试视角。是测试对质量负责的手段。

单元测试，意味着开发成本显著增加，也许开发和测试比是1：1。但在业务变化比较频繁的场合下，单元测试代码会明显存在开发动力不足，改动业务同时改造单元测试用例的内容会让人不爽。

很多面向测试编程的项目，容易流产实施不下去，很多的原因是单元功能变化太多，引起大量测试用例的修改。

相对来说，面向业务接口的测试，变更的几率相对更少，更稳定。可以把业务功能做成黑盒子，业务数据验证也通过接口实施，会进一步减少测试用例变更的成本。

### 接口自动化 还是 UI自动化
这也是自动化测试里比较通用的两个分类。

接口自动化，一般来说都是面向数据接口（HTTP，TCP）等，是一个比较用数据思维描述的内容。实现难度低，行业里比较成熟。
UI自动化，由于是操作层面，受限于使用终端的技术，以及不可控的用户操作功能，这个实现难度相对高。相对来说更为新兴一些，这两年行业里也有着蓬勃发展，很多AI技术也应用其中。很多大厂也投入研发一些AI的机器人，用来模拟人工操作的动作，更好的提升系统质量。

如果从28法则来说，实施接口自动化的投入产出比会更好一些，会更好的看到系统质量的提升。

### 实施成本
自动化测试框架有很多种，例如Selenium， Allure，很多大厂也有自研的。
拿Jenkins+Allure+Python为例：
- Jenkins+Allure的环境，无论是容器化还是物理机。
- 部署系统的钩子
- 功能测试脚本
- 能写脚本的人员

#### 冒烟测试
！！！！！没有冒烟测试！！！！
测试环节更友善，失败率低。
一个不高的开发质量，会造成大量的测试时间浪费，从构建数据，测试验证，投入一次测试资源。失败意味着测试时间的浪费，后面影响其他功能的迭代。

- 开发人员通过冒烟测试
- 测试人员完善冒烟测试

#### 流程的转变
需求-开发-测试-部署
需求-开发&测试-部署-测试

#### 实施环节
- 开发环节通过冒烟测试
- 固定分支部署流程后自动化回归测试
- 约定时间执行压力测试。

### 接口自动化的改善关注指标
- 优化类项目的测试成本。
- 系统故障和接口测试数量。

### 如何评估收益
测试自动化是个随着功能覆盖度上升，效能逐渐提升的。
项目初期会面临时间成本变长的问题，比单纯的测试，大量的数据准备的代码落地要求，要远超过单纯的功能测试人工操作时间。
1. 测试开发比。
2. 测试事故率。